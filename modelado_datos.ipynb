{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<h2><font color=\"#004D7F\" size=6>Modelado de datos</font></h2>\n",
    "<h1></h1>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<font color=\"#004D7F\" size=5>Curso: Inteligencia Artificial</font><br>\n",
    "<font color=\"#004D7F\" size=5>Docente: Juan Villegas Cubas</font><br>\n",
    "    <font color=\"#004D7F\" size=5>Alumno: Gonzalo López Guerrero</font><br>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Noviembre 2020</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14441, 15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import plotly \n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "\n",
    "#import twitter data\n",
    "\n",
    "#PATH =\"/content/drive/MyDrive/IA_RNN/Data\"\n",
    "PATH =\"../Data\"\n",
    "filename = PATH+'/Tweets.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "print(dataset.shape)#rows x columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9095\n",
       "neutral     3034\n",
       "positive    2312\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cantidad de tweet por clase\n",
    "dataset['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir a entero los valores de las clases\n",
    "dataset['airline_sentiment'] = dataset['airline_sentiment'].replace('neutral', 1)\n",
    "dataset['airline_sentiment'] = dataset['airline_sentiment'].replace('negative', 0)\n",
    "dataset['airline_sentiment'] = dataset['airline_sentiment'].replace('positive', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2\n",
       "1        0\n",
       "2        2\n",
       "3        1\n",
       "4        2\n",
       "        ..\n",
       "14436    2\n",
       "14437    0\n",
       "14438    1\n",
       "14439    0\n",
       "14440    1\n",
       "Name: airline_sentiment, Length: 14441, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividar los datos entre text y clase\n",
    "X = dataset['text'] # data\n",
    "y = dataset['airline_sentiment'] # labels\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "#class_weight = class_weight.compute_class_weight('balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Procesando Datos<7h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15615"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  #reemplazar los caracteres especiales\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence #analiza la secuencia de los caracteres\n",
    "# Convertisa los datos en token\n",
    "# create tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "# Encuentra la cantidad de palabras únicas en los tweets\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# indexar las palabras (relacionar con su par entero)\n",
    "sequences = t.texts_to_sequences(X)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mayor numero de caracteres en un tweet  30\n"
     ]
    }
   ],
   "source": [
    "# Encuentra el tweet con mayor numero de caracteres para ser tomado como referencias \n",
    "#al momento de vectorizar a las palabras\n",
    "def max_tweet():\n",
    "    for i in range(1, len(sequences)):\n",
    "        max_length = len(sequences[0])\n",
    "        if len(sequences[i]) > max_length:\n",
    "            max_length = len(sequences[i])\n",
    "    return max_length\n",
    "tweet_num = max_tweet()\n",
    "print(\"Mayor numero de caracteres en un tweet \" , tweet_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123,   3, 164, ...,   0,   0,   0],\n",
       "       [123,  21, 262, ...,   0,   0,   0],\n",
       "       [123, 293,  70, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 13,  73, 675, ...,   0,   0,   0],\n",
       "       [ 13,   6,  22, ...,   0,   0,   0],\n",
       "       [ 13,  41,  22, ...,   2, 179,   8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# las palabras no encontradas en el tweet se llenan con ceros en el array\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = tweet_num\n",
    "padded_X = pad_sequences(sequences, padding='post', maxlen=maxlen)\n",
    "valor_maximo =np.amax(padded_X) \n",
    "padded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convertir las etiquestas\n",
    "labels = to_categorical(np.asarray(y))\n",
    "labels\n",
    "# 1 => representa la clase que representa negativo / neutral / positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataset.values\n",
    "padded_Y = array[:,1] #array de clases\n",
    "padded_Y_test=padded_Y.astype('int')\n",
    "#padded_Y\n",
    "padded_X_test = (padded_X /valor_maximo) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (11552, 30)\n",
      "y_train size: (11552, 3)\n",
      "X_test size: (2889, 30)\n",
      "y_test size: (2889, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X, labels, test_size = 0.2, random_state = 0)\n",
    "# Size of train and test datasets\n",
    "print('X_train size:', X_train.shape)\n",
    "print('y_train size:', y_train.shape)\n",
    "print('X_test size:', X_test.shape)\n",
    "print('y_test size:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectores de palabras cargados: 400000\n"
     ]
    }
   ],
   "source": [
    "# GloVe se define como un “algoritmo de aprendizaje no supervisado \n",
    "#para obtener representaciones vectoriales de palabras”. \n",
    "#Descargamos datos del sitio web vinculado y usamos específicamente las incrustaciones\n",
    "#100-dimensionales de 400k palabras de Wikipedia en inglés en 2014. \n",
    "#Esto se representa en un archivo txt que debemos analizar para crear un\n",
    "#índice que mapee las palabras a su representación vectorial.\n",
    "\n",
    "# 100 dimensional version (embedding dimension)\n",
    "\n",
    "#ayuda a en la decodificacion\n",
    "#https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "embeddings_index = dict()\n",
    "f = open(PATH+'/glove/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Vectores de palabras cargados: %s' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15615, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se Obtiene todas las palabras únicas en nuestro conjunto de entrenamiento: índice de tokenizadores\n",
    "# Se encuentra el vector de peso correspondiente en la incrustación de GloVe\n",
    "\n",
    "# Se define el tamaño de la matriz de incrustación: número de palabras únicas x incrustación dim (100)\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "# llenado de la matrix\n",
    "for word, i in t.word_index.items():  # dictionary\n",
    "    embedding_vector = embeddings_index.get(word) # obtiene un vector incrustado de palabra de GloVe\n",
    "    if embedding_vector is not None:\n",
    "        # se agrega a la matrix\n",
    "        embedding_matrix[i] = embedding_vector # cada fila de la matrix\n",
    "\n",
    "#print(embedding_matrix)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.18970001,  0.050024  ,  0.19084001, ..., -0.39804   ,\n",
       "         0.47646999, -0.15983   ],\n",
       "       [-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
       "         0.82779998,  0.27061999],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una capa de incrustación usando una matriz de incrustación\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# la entrada es vocab_size, la salida es 100\n",
    "# pesos de la matriz de incrustación, establezca entrenable = Falso || para q no se modifiquen los valores de Glove\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix],\n",
    "                           input_length = tweet_num, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1: Modelo LSTM simple con regularización, aumento de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 100)           1561500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 1,927,839\n",
      "Trainable params: 366,339\n",
      "Non-trainable params: 1,561,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_mod1 = Sequential()\n",
    "lstm_mod1.add(embedding_layer)\n",
    "lstm_mod1.add(LSTM(256, \n",
    "               dropout = 0.4,\n",
    "               recurrent_dropout = 0.6))\n",
    "lstm_mod1.add(Dense(3, activation='softmax'))\n",
    "lstm_mod1.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['acc'])\n",
    "lstm_mod1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 19s 508ms/step - loss: 0.8576 - acc: 0.6215 - val_loss: 0.8208 - val_acc: 0.6621\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 0.7475 - acc: 0.6979 - val_loss: 0.6774 - val_acc: 0.7278\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 17s 460ms/step - loss: 0.6506 - acc: 0.7401 - val_loss: 0.6272 - val_acc: 0.7434\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 19s 523ms/step - loss: 0.6287 - acc: 0.7402 - val_loss: 0.5949 - val_acc: 0.7542\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 21s 564ms/step - loss: 0.6012 - acc: 0.7587 - val_loss: 0.6102 - val_acc: 0.7624\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 21s 573ms/step - loss: 0.5843 - acc: 0.7624 - val_loss: 0.6190 - val_acc: 0.7572\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 21s 568ms/step - loss: 0.5784 - acc: 0.7651 - val_loss: 0.5541 - val_acc: 0.7741\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 20s 536ms/step - loss: 0.5652 - acc: 0.7690 - val_loss: 0.5668 - val_acc: 0.7728\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 20s 530ms/step - loss: 0.5500 - acc: 0.7791 - val_loss: 0.5398 - val_acc: 0.7836\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 20s 537ms/step - loss: 0.5399 - acc: 0.7840 - val_loss: 0.5346 - val_acc: 0.7854\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 25s 673ms/step - loss: 0.5333 - acc: 0.7815 - val_loss: 0.5615 - val_acc: 0.7672\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 22s 596ms/step - loss: 0.5166 - acc: 0.7922 - val_loss: 0.5300 - val_acc: 0.7914\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 22s 592ms/step - loss: 0.5097 - acc: 0.7935 - val_loss: 0.5576 - val_acc: 0.7659\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 25s 677ms/step - loss: 0.5107 - acc: 0.7922 - val_loss: 0.5266 - val_acc: 0.7966\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 27s 733ms/step - loss: 0.4767 - acc: 0.8102 - val_loss: 0.5469 - val_acc: 0.7932\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 23s 616ms/step - loss: 0.4683 - acc: 0.8143 - val_loss: 0.5188 - val_acc: 0.7888\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 23s 615ms/step - loss: 0.4708 - acc: 0.8106 - val_loss: 0.5359 - val_acc: 0.7823\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 23s 613ms/step - loss: 0.4585 - acc: 0.8170 - val_loss: 0.5323 - val_acc: 0.7910\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 23s 633ms/step - loss: 0.4489 - acc: 0.8193 - val_loss: 0.5421 - val_acc: 0.7871\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 23s 631ms/step - loss: 0.4404 - acc: 0.8245 - val_loss: 0.5359 - val_acc: 0.7828\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 22s 595ms/step - loss: 0.4405 - acc: 0.8249 - val_loss: 0.5149 - val_acc: 0.8005\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 23s 614ms/step - loss: 0.4101 - acc: 0.8353 - val_loss: 0.5185 - val_acc: 0.7893\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 22s 603ms/step - loss: 0.4054 - acc: 0.8403 - val_loss: 0.7214 - val_acc: 0.7231\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.4189 - acc: 0.8343 - val_loss: 0.5194 - val_acc: 0.7988\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 22s 587ms/step - loss: 0.3923 - acc: 0.8441 - val_loss: 0.5733 - val_acc: 0.8005\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 24s 641ms/step - loss: 0.3778 - acc: 0.8523 - val_loss: 0.5355 - val_acc: 0.8044\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 24s 641ms/step - loss: 0.3704 - acc: 0.8529 - val_loss: 0.5462 - val_acc: 0.7966\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 22s 603ms/step - loss: 0.3680 - acc: 0.8572 - val_loss: 0.5607 - val_acc: 0.8040\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 25s 669ms/step - loss: 0.3541 - acc: 0.8586 - val_loss: 0.5358 - val_acc: 0.8005\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 22s 595ms/step - loss: 0.3515 - acc: 0.8635 - val_loss: 0.5582 - val_acc: 0.7927\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 21s 558ms/step - loss: 0.3397 - acc: 0.8667 - val_loss: 0.5809 - val_acc: 0.8014\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 21s 569ms/step - loss: 0.3185 - acc: 0.8757 - val_loss: 0.6045 - val_acc: 0.7785\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 23s 616ms/step - loss: 0.3129 - acc: 0.8785 - val_loss: 0.5535 - val_acc: 0.7997\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 20s 528ms/step - loss: 0.3143 - acc: 0.8791 - val_loss: 0.6213 - val_acc: 0.7945\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 19s 513ms/step - loss: 0.2963 - acc: 0.8823 - val_loss: 0.5987 - val_acc: 0.7949\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 19s 502ms/step - loss: 0.3314 - acc: 0.8707 - val_loss: 0.5894 - val_acc: 0.7979\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 18s 498ms/step - loss: 0.2826 - acc: 0.8938 - val_loss: 0.6208 - val_acc: 0.7927\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 19s 508ms/step - loss: 0.2908 - acc: 0.8856 - val_loss: 0.5811 - val_acc: 0.7880\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 19s 515ms/step - loss: 0.2727 - acc: 0.8938 - val_loss: 0.6649 - val_acc: 0.7971\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 19s 501ms/step - loss: 0.2685 - acc: 0.9003 - val_loss: 0.6372 - val_acc: 0.7871\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 18s 493ms/step - loss: 0.2447 - acc: 0.9036 - val_loss: 0.6815 - val_acc: 0.7772\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 19s 515ms/step - loss: 0.2561 - acc: 0.9012 - val_loss: 0.7778 - val_acc: 0.7919\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 18s 498ms/step - loss: 0.2348 - acc: 0.9115 - val_loss: 0.7033 - val_acc: 0.7823\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 18s 498ms/step - loss: 0.2322 - acc: 0.9128 - val_loss: 0.6826 - val_acc: 0.7966\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 18s 499ms/step - loss: 0.2349 - acc: 0.9074 - val_loss: 0.6448 - val_acc: 0.8010\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 19s 507ms/step - loss: 0.2169 - acc: 0.9164 - val_loss: 0.7015 - val_acc: 0.7932\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 18s 495ms/step - loss: 0.2125 - acc: 0.9188 - val_loss: 0.7184 - val_acc: 0.7958\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.1922 - acc: 0.9259 - val_loss: 0.7658 - val_acc: 0.8027\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 19s 525ms/step - loss: 0.1872 - acc: 0.9307 - val_loss: 0.7224 - val_acc: 0.7949\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 20s 540ms/step - loss: 0.1815 - acc: 0.9324 - val_loss: 0.7509 - val_acc: 0.7984\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 21s 558ms/step - loss: 0.1755 - acc: 0.9350 - val_loss: 0.7862 - val_acc: 0.7854\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.1759 - acc: 0.9341 - val_loss: 0.7460 - val_acc: 0.7832\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 20s 539ms/step - loss: 0.1735 - acc: 0.9357 - val_loss: 0.8640 - val_acc: 0.7901\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 26s 710ms/step - loss: 0.1683 - acc: 0.9364 - val_loss: 0.8580 - val_acc: 0.7906\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 22s 597ms/step - loss: 0.1601 - acc: 0.9409 - val_loss: 0.8038 - val_acc: 0.7945\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 24s 661ms/step - loss: 0.1577 - acc: 0.9409 - val_loss: 0.8251 - val_acc: 0.7871\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 21s 580ms/step - loss: 0.1466 - acc: 0.9482 - val_loss: 0.7712 - val_acc: 0.7871\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 23s 608ms/step - loss: 0.1552 - acc: 0.9441 - val_loss: 0.7734 - val_acc: 0.7893\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 24s 645ms/step - loss: 0.1352 - acc: 0.9502 - val_loss: 0.7995 - val_acc: 0.7919\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 29s 773ms/step - loss: 0.1411 - acc: 0.9471 - val_loss: 0.8181 - val_acc: 0.7914\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 26s 711ms/step - loss: 0.1300 - acc: 0.9534 - val_loss: 0.8161 - val_acc: 0.7880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "37/37 [==============================] - 23s 609ms/step - loss: 0.1355 - acc: 0.9491 - val_loss: 0.8435 - val_acc: 0.7897\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 24s 643ms/step - loss: 0.1527 - acc: 0.9433 - val_loss: 0.8321 - val_acc: 0.7936\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 26s 695ms/step - loss: 0.1516 - acc: 0.9443 - val_loss: 0.8020 - val_acc: 0.7984\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 21s 566ms/step - loss: 0.1192 - acc: 0.9580 - val_loss: 0.9038 - val_acc: 0.7785\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 22s 591ms/step - loss: 0.1237 - acc: 0.9562 - val_loss: 0.8620 - val_acc: 0.7884\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 23s 609ms/step - loss: 0.1147 - acc: 0.9582 - val_loss: 0.9649 - val_acc: 0.7815\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 23s 615ms/step - loss: 0.1076 - acc: 0.9613 - val_loss: 0.9466 - val_acc: 0.7845\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 21s 576ms/step - loss: 0.1042 - acc: 0.9609 - val_loss: 1.0449 - val_acc: 0.7875\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 23s 621ms/step - loss: 0.0982 - acc: 0.9642 - val_loss: 0.9798 - val_acc: 0.7849\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 22s 587ms/step - loss: 0.0976 - acc: 0.9632 - val_loss: 0.9439 - val_acc: 0.7893\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 19s 504ms/step - loss: 0.0985 - acc: 0.9648 - val_loss: 0.9183 - val_acc: 0.7888\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 18s 486ms/step - loss: 0.0961 - acc: 0.9656 - val_loss: 0.9787 - val_acc: 0.7992\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 19s 500ms/step - loss: 0.0992 - acc: 0.9646 - val_loss: 1.0698 - val_acc: 0.7884\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 18s 500ms/step - loss: 0.1004 - acc: 0.9631 - val_loss: 1.0084 - val_acc: 0.7893\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 20s 539ms/step - loss: 0.0878 - acc: 0.9683 - val_loss: 0.9956 - val_acc: 0.7901\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 20s 535ms/step - loss: 0.0873 - acc: 0.9714 - val_loss: 1.0356 - val_acc: 0.7927\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 19s 507ms/step - loss: 0.0860 - acc: 0.9708 - val_loss: 0.9949 - val_acc: 0.7901\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 20s 541ms/step - loss: 0.0890 - acc: 0.9675 - val_loss: 0.9582 - val_acc: 0.7785\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 22s 603ms/step - loss: 0.0926 - acc: 0.9676 - val_loss: 1.0247 - val_acc: 0.7832\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 19s 511ms/step - loss: 0.0936 - acc: 0.9684 - val_loss: 0.9989 - val_acc: 0.7897\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 19s 524ms/step - loss: 0.0786 - acc: 0.9719 - val_loss: 0.9483 - val_acc: 0.7893\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 22s 584ms/step - loss: 0.0761 - acc: 0.9742 - val_loss: 1.0281 - val_acc: 0.7906\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.0713 - acc: 0.9739 - val_loss: 1.1059 - val_acc: 0.7797\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 20s 528ms/step - loss: 0.0754 - acc: 0.9742 - val_loss: 0.9917 - val_acc: 0.7914\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 23s 617ms/step - loss: 0.1009 - acc: 0.9632 - val_loss: 1.0400 - val_acc: 0.7845\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 19s 510ms/step - loss: 0.0857 - acc: 0.9686 - val_loss: 0.9623 - val_acc: 0.7945\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 19s 511ms/step - loss: 0.0718 - acc: 0.9758 - val_loss: 1.0337 - val_acc: 0.7901\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 19s 527ms/step - loss: 0.0747 - acc: 0.9745 - val_loss: 1.1374 - val_acc: 0.7893\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 19s 504ms/step - loss: 0.0792 - acc: 0.9735 - val_loss: 1.0477 - val_acc: 0.7906\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 19s 508ms/step - loss: 0.0707 - acc: 0.9759 - val_loss: 1.0439 - val_acc: 0.7871\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 19s 518ms/step - loss: 0.0607 - acc: 0.9781 - val_loss: 1.1555 - val_acc: 0.7897\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 19s 513ms/step - loss: 0.0633 - acc: 0.9775 - val_loss: 1.0491 - val_acc: 0.7858\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 19s 521ms/step - loss: 0.0729 - acc: 0.9735 - val_loss: 1.0883 - val_acc: 0.7888\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 19s 516ms/step - loss: 0.0829 - acc: 0.9711 - val_loss: 1.0982 - val_acc: 0.7945\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 19s 521ms/step - loss: 0.0691 - acc: 0.9784 - val_loss: 1.0388 - val_acc: 0.7949\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 19s 519ms/step - loss: 0.0681 - acc: 0.9768 - val_loss: 1.1026 - val_acc: 0.7919\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 21s 558ms/step - loss: 0.0570 - acc: 0.9805 - val_loss: 1.1839 - val_acc: 0.7897\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 21s 568ms/step - loss: 0.0614 - acc: 0.9787 - val_loss: 1.1099 - val_acc: 0.7875\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 24s 659ms/step - loss: 0.0558 - acc: 0.9804 - val_loss: 1.1309 - val_acc: 0.7927\n"
     ]
    }
   ],
   "source": [
    "hist_1 = lstm_mod1.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y test accuracy\n",
    "loss, accuracy = lstm_mod1.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = lstm_mod1.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train/test loss and accuracy\n",
    "acc = hist_1.history['acc']\n",
    "val_acc = hist_1.history['val_acc']\n",
    "loss = hist_1.history['loss']\n",
    "val_loss = hist_1.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "# Get predicted values\n",
    "y_pred = lstm_mod1.predict(X_test)  # outputs probabilities of each sentiment\n",
    "# Create empty numpy array to match length of training observations\n",
    "y_pred_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with highest probability\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    label_predict = np.argmax(y_pred[i]) # column with max probability\n",
    "    y_pred_array[i] = label_predict\n",
    "\n",
    "# convert to integers\n",
    "y_pred_array = y_pred_array.astype(int)\n",
    "# Convert y_test to 1d numpy array\n",
    "y_test_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with 1\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    label_predict = np.argmax(y_test[i])\n",
    "    y_test_array[i] = label_predict\n",
    "\n",
    "y_test_array = y_test_array.astype(int)\n",
    "class_names = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2: LSTM con regularización, reducir dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mod2 = Sequential()\n",
    "lstm_mod2.add(embedding_layer)\n",
    "lstm_mod2.add(LSTM(64, \n",
    "               dropout = 0.4,\n",
    "               recurrent_dropout = 0.6))\n",
    "lstm_mod2.add(Dense(3, activation='softmax'))\n",
    "lstm_mod2.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['acc'])\n",
    "lstm_mod2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_2 = lstm_mod2.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datis de entrenamiento y test\n",
    "loss, accuracy = lstm_mod2.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = lstm_mod2.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train/test loss and accuracy\n",
    "acc = hist_2.history['acc']\n",
    "val_acc = hist_2.history['val_acc']\n",
    "loss = hist_2.history['loss']\n",
    "val_loss = hist_2.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "# Get predicted values\n",
    "y_pred = lstm_mod2.predict(X_test)  # outputs probabilities of each sentiment\n",
    "# Create empty numpy array to match length of training observations\n",
    "y_pred_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with highest probability\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    label_predict = np.argmax(y_pred[i]) # column with max probability\n",
    "    y_pred_array[i] = label_predict\n",
    "\n",
    "# convert to integers\n",
    "y_pred_array = y_pred_array.astype(int)\n",
    "# Convert y_test to 1d numpy array\n",
    "y_test_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with 1\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    label_predict = np.argmax(y_test[i])\n",
    "    y_test_array[i] = label_predict\n",
    "\n",
    "y_test_array = y_test_array.astype(int)\n",
    "class_names = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 3: Apilamiento de capas LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model_3 = Sequential()\n",
    "model_3.add(embedding_layer)\n",
    "model_3.add(LSTM(256, \n",
    "               dropout = 0.4, \n",
    "               recurrent_dropout = 0.6,\n",
    "                 return_sequences = True))#solo aplicable a multicapas\n",
    "model_3.add(LSTM(128,\n",
    "                dropout = 0.4,\n",
    "                recurrent_dropout = 0.6))\n",
    "model_3.add(Dense(3, activation='softmax'))\n",
    "model_3.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_3 = model_3.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_3.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_3.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train/test loss and accuracy\n",
    "acc = hist_3.history['acc']\n",
    "val_acc = hist_3.history['val_acc']\n",
    "loss = hist_3.history['loss']\n",
    "val_loss = hist_3.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "# Get predicted values\n",
    "y_pred = model_3.predict(X_test)  # outputs probabilities of each sentiment\n",
    "# Create empty numpy array to match length of training observations\n",
    "y_pred_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with highest probability\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    label_predict = np.argmax(y_pred[i]) # column with max probability\n",
    "    y_pred_array[i] = label_predict\n",
    "\n",
    "# convert to integers\n",
    "y_pred_array = y_pred_array.astype(int)\n",
    "# Convert y_test to 1d numpy array\n",
    "y_test_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with 1\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    label_predict = np.argmax(y_test[i])\n",
    "    y_test_array[i] = label_predict\n",
    "\n",
    "y_test_array = y_test_array.astype(int)\n",
    "class_names = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 4: Apilamiento de capas GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model 2:\n",
    "model_4 = Sequential()\n",
    "model_4.add(embedding_layer)\n",
    "model_4.add(GRU(256, \n",
    "               dropout = 0.4, \n",
    "               recurrent_dropout = 0.6,\n",
    "                 return_sequences = True))\n",
    "model_4.add(GRU(128,\n",
    "                dropout = 0.4,\n",
    "                recurrent_dropout = 0.6))\n",
    "model_4.add(Dense(3, activation='softmax'))\n",
    "model_4.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_4 = model_4.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=200, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_4.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_4.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train/test loss and accuracy\n",
    "acc = hist_4.history['acc']\n",
    "val_acc = hist_4.history['val_acc']\n",
    "loss = hist_4.history['loss']\n",
    "val_loss = hist_4.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "# Get predicted values\n",
    "y_pred = model_4.predict(X_test)  # outputs probabilities of each sentiment\n",
    "# Create empty numpy array to match length of training observations\n",
    "y_pred_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with highest probability\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    label_predict = np.argmax(y_pred[i]) # column with max probability\n",
    "    y_pred_array[i] = label_predict\n",
    "\n",
    "# convert to integers\n",
    "y_pred_array = y_pred_array.astype(int)\n",
    "# Convert y_test to 1d numpy array\n",
    "y_test_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with 1\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    label_predict = np.argmax(y_test[i])\n",
    "    y_test_array[i] = label_predict\n",
    "\n",
    "y_test_array = y_test_array.astype(int)\n",
    "class_names = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizar :Model 4:-Reduced GRU with More Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Sequential()\n",
    "model_5.add(embedding_layer)\n",
    "model_5.add(GRU(256, \n",
    "               dropout = 0.3, \n",
    "               recurrent_dropout = 0.5,\n",
    "                 return_sequences = True))\n",
    "model_5.add(GRU(128,\n",
    "                dropout = 0.2,\n",
    "                recurrent_dropout = 0.5))\n",
    "model_5.add(Dense(3, activation='softmax'))\n",
    "model_5.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_5 = model_5.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_5.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_5.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train/test loss and accuracy\n",
    "acc = hist_5.history['acc']\n",
    "val_acc = hist_5.history['val_acc']\n",
    "loss = hist_5.history['loss']\n",
    "val_loss = hist_5.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "# Get predicted values\n",
    "y_pred = model_5.predict(X_test)  # outputs probabilities of each sentiment\n",
    "# Create empty numpy array to match length of training observations\n",
    "y_pred_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with highest probability\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    label_predict = np.argmax(y_pred[i]) # column with max probability\n",
    "    y_pred_array[i] = label_predict\n",
    "\n",
    "# convert to integers\n",
    "y_pred_array = y_pred_array.astype(int)\n",
    "# Convert y_test to 1d numpy array\n",
    "y_test_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with 1\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    label_predict = np.argmax(y_test[i])\n",
    "    y_test_array[i] = label_predict\n",
    "\n",
    "y_test_array = y_test_array.astype(int)\n",
    "class_names = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test_array, y_pred_array, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <font color=\"#004D7F\">Comparación de Modelos</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clave para una comparación equitativa de los algoritmos de ML es garantizar que cada algoritmo se evalúe de la misma manera en los mismos datos. Los algoritmos se comparan en un único conjunto de datos:\n",
    "* Logistic Regression.\n",
    "* Linear Discriminant Analysis.\n",
    "* k-Nearest Neighbors.\n",
    "* Classification and Regression Trees. \n",
    "* Naive Bayes.\n",
    "* Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "models.append((\"Modelo LSTM simple con regularización, aumento de dimensionalidad\",  \n",
    "               lstm_mod1.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"LSTM con regularización, reducir dimensionalidad\",  \n",
    "               lstm_mod2.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"Apilamiento de capas LSTM\",  \n",
    "               model_3.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"Apilamiento de capas GRU\", \n",
    "               model_4.evaluate(X_train, y_train, verbose=False)))\n",
    "\n",
    "results=[]\n",
    "names=[]\n",
    "\n",
    "\n",
    "for name, model in models: \n",
    "    loss, accuracy = model\n",
    "    results.append(accuracy)\n",
    "    names.append(name)\n",
    "    \n",
    "    print(f\"Accuracy: {name}: {accuracy*100.0:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
