{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<h2><font color=\"#004D7F\" size=6>Modelado de datos</font></h2>\n",
    "<h1></h1>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<font color=\"#004D7F\" size=5>Curso: Inteligencia Artificial</font><br>\n",
    "<font color=\"#004D7F\" size=5>Docente: Juan Villegas Cubas</font><br>\n",
    "    <font color=\"#004D7F\" size=5>Alumno: Gonzalo López Guerrero</font><br>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Noviembre 2020</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import plotly \n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "\n",
    "#import twitter data\n",
    "filename = '../Data/Tweets.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "print(dataset.shape)#rows x columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cantidad de tweet por clase\n",
    "dataset['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir a entero los valores de las clases\n",
    "dataset['airline_sentiment'] = dataset['airline_sentiment'].replace('neutral', 1)\n",
    "dataset['airline_sentiment'] = dataset['airline_sentiment'].replace('negative', 0)\n",
    "dataset['airline_sentiment'] = dataset['airline_sentiment'].replace('positive', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividar los datos entre text y clase\n",
    "X = dataset['text'] # data\n",
    "y = dataset['airline_sentiment'] # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Procesando Datos<7h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  #reemplazar los caracteres especiales\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence #analiza la secuencia de los caracteres\n",
    "# Convertisa los datos en token\n",
    "# create tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "# Encuentra la cantidad de palabras únicas en los tweets\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# indexar las palabras (relacionar con su par entero)\n",
    "sequences = t.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mayor numero de caracteres en un tweet  30\n"
     ]
    }
   ],
   "source": [
    "# Encuentra el tweet con mayor numero de caracteres para ser tomado como referencias \n",
    "#al momento de vectorizar a las palabras\n",
    "def max_tweet():\n",
    "    for i in range(1, len(sequences)):\n",
    "        max_length = len(sequences[0])\n",
    "        if len(sequences[i]) > max_length:\n",
    "            max_length = len(sequences[i])\n",
    "    return max_length\n",
    "tweet_num = max_tweet()\n",
    "print(\"Mayor numero de caracteres en un tweet \" , tweet_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  81,   62, 6686, ...,    0,    0,    0],\n",
       "       [  81,  558,  590, ...,    0,    0,    0],\n",
       "       [  81,    3,  207, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  13,   75,  661, ...,    0,    0,    0],\n",
       "       [  13,    6,   22, ...,    0,    0,    0],\n",
       "       [  13,   41,   22, ...,    2,  179,    8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# las palabras no encontradas en el tweet se llenan con ceros en el array\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = tweet_num\n",
    "padded_X = pad_sequences(sequences, padding='post', maxlen=maxlen)\n",
    "valor_maximo =np.amax(padded_X) \n",
    "padded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convertir las etiquestas\n",
    "labels = to_categorical(np.asarray(y))\n",
    "labels\n",
    "# 1 => representa la clase que representa negativo / neutral / positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataset.values\n",
    "padded_Y = array[:,1] #array de clases\n",
    "padded_Y_test=padded_Y.astype('int')\n",
    "#padded_Y\n",
    "padded_X_test = (padded_X /valor_maximo) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (11712, 30)\n",
      "y_train size: (11712, 3)\n",
      "X_test size: (2928, 30)\n",
      "y_test size: (2928, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X, labels, test_size = 0.2, random_state = 0)\n",
    "# Size of train and test datasets\n",
    "print('X_train size:', X_train.shape)\n",
    "print('y_train size:', y_train.shape)\n",
    "print('X_test size:', X_test.shape)\n",
    "print('y_test size:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectores de palabras cargados: 400000\n"
     ]
    }
   ],
   "source": [
    "# GloVe se define como un “algoritmo de aprendizaje no supervisado \n",
    "#para obtener representaciones vectoriales de palabras”. \n",
    "#Descargamos datos del sitio web vinculado y usamos específicamente las incrustaciones\n",
    "#100-dimensionales de 400k palabras de Wikipedia en inglés en 2014. \n",
    "#Esto se representa en un archivo txt que debemos analizar para crear un\n",
    "#índice que mapee las palabras a su representación vectorial.\n",
    "\n",
    "# 100 dimensional version (embedding dimension)\n",
    "\n",
    "#ayuda a en la decodificacion\n",
    "#https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "embeddings_index = dict()\n",
    "f = open('../Data/glove/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Vectores de palabras cargados: %s' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15769, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se Obtiene todas las palabras únicas en nuestro conjunto de entrenamiento: índice de tokenizadores\n",
    "# Se encuentra el vector de peso correspondiente en la incrustación de GloVe\n",
    "\n",
    "# Se define el tamaño de la matriz de incrustación: número de palabras únicas x incrustación dim (100)\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "# llenado de la matrix\n",
    "for word, i in t.word_index.items():  # dictionary\n",
    "    embedding_vector = embeddings_index.get(word) # obtiene un vector incrustado de palabra de GloVe\n",
    "    if embedding_vector is not None:\n",
    "        # se agrega a la matrix\n",
    "        embedding_matrix[i] = embedding_vector # cada fila de la matrix\n",
    "\n",
    "#print(embedding_matrix)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una capa de incrustación usando una matriz de incrustación\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# la entrada es vocab_size, la salida es 100\n",
    "# pesos de la matriz de incrustación, establezca entrenable = Falso || para q no se modifiquen los valores de Glove\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix],\n",
    "                           input_length = tweet_num, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple LSTM Model with regularization, increase dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 100)           1576900   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 1,943,239\n",
      "Trainable params: 366,339\n",
      "Non-trainable params: 1,576,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_mod1 = Sequential()\n",
    "lstm_mod1.add(embedding_layer)\n",
    "lstm_mod1.add(LSTM(256, \n",
    "               dropout = 0.2, \n",
    "               recurrent_dropout = 0.5))\n",
    "lstm_mod1.add(Dense(3, activation='softmax'))\n",
    "lstm_mod1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "lstm_mod1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 17s 464ms/step - loss: 0.8469 - acc: 0.6235 - val_loss: 0.7914 - val_acc: 0.6773\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 22s 584ms/step - loss: 0.6963 - acc: 0.7098 - val_loss: 0.6303 - val_acc: 0.7499\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 22s 585ms/step - loss: 0.6375 - acc: 0.7363 - val_loss: 0.5993 - val_acc: 0.7580\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 21s 574ms/step - loss: 0.6249 - acc: 0.7451 - val_loss: 0.6013 - val_acc: 0.7614\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 22s 585ms/step - loss: 0.5941 - acc: 0.7611 - val_loss: 0.5629 - val_acc: 0.7746\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 21s 561ms/step - loss: 0.5803 - acc: 0.7616 - val_loss: 0.5838 - val_acc: 0.7670\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 21s 574ms/step - loss: 0.5741 - acc: 0.7633 - val_loss: 0.5556 - val_acc: 0.7853\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 21s 568ms/step - loss: 0.5672 - acc: 0.7715 - val_loss: 0.5845 - val_acc: 0.7644\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 21s 565ms/step - loss: 0.5516 - acc: 0.7750 - val_loss: 0.5366 - val_acc: 0.7892\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 21s 568ms/step - loss: 0.5268 - acc: 0.7861 - val_loss: 0.5353 - val_acc: 0.7934\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 24s 659ms/step - loss: 0.5309 - acc: 0.7850 - val_loss: 0.5576 - val_acc: 0.7648\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 25s 669ms/step - loss: 0.5073 - acc: 0.7985 - val_loss: 0.5500 - val_acc: 0.7810\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 26s 690ms/step - loss: 0.5043 - acc: 0.7939 - val_loss: 0.5393 - val_acc: 0.7943\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 27s 733ms/step - loss: 0.4912 - acc: 0.8041 - val_loss: 0.5124 - val_acc: 0.7990\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 21s 573ms/step - loss: 0.4806 - acc: 0.8061 - val_loss: 0.5727 - val_acc: 0.7917\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 27s 728ms/step - loss: 0.4699 - acc: 0.8137 - val_loss: 0.5314 - val_acc: 0.7985\n",
      "Epoch 17/100\n",
      "33/37 [=========================>....] - ETA: 2s - loss: 0.4554 - acc: 0.8204"
     ]
    }
   ],
   "source": [
    "hist_1 = lstm_mod1.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y test accuracy\n",
    "loss, accuracy = lstm_mod1.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = lstm_mod1.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LSTM with regularization, reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mod2 = Sequential()\n",
    "lstm_mod2.add(embedding_layer)\n",
    "lstm_mod2.add(LSTM(64, \n",
    "               dropout = 0.2, \n",
    "               recurrent_dropout = 0.5))\n",
    "lstm_mod2.add(Dense(3, activation='softmax'))\n",
    "lstm_mod2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "lstm_mod2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_2 = lstm_mod2.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find train and test accuracy\n",
    "loss, accuracy = lstm_mod2.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = lstm_mod2.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: LSTM Layer Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model_3 = Sequential()\n",
    "model_3.add(embedding_layer)\n",
    "model_3.add(LSTM(256, \n",
    "               dropout = 0.2, \n",
    "               recurrent_dropout = 0.5,\n",
    "                 return_sequences = True))\n",
    "model_3.add(LSTM(128,\n",
    "                dropout = 0.2,\n",
    "                recurrent_dropout = 0.5))\n",
    "model_3.add(Dense(3, activation='softmax'))\n",
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3 = model_3.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find train and test accuracy\n",
    "loss, accuracy = model_3.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_3.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: GRU Layer Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model 2:\n",
    "model_4 = Sequential()\n",
    "model_4.add(embedding_layer)\n",
    "model_4.add(GRU(256, \n",
    "               dropout = 0.2, \n",
    "               recurrent_dropout = 0.5,\n",
    "                 return_sequences = True))\n",
    "model_4.add(GRU(128,\n",
    "                dropout = 0.2,\n",
    "                recurrent_dropout = 0.5))\n",
    "model_4.add(Dense(3, activation='softmax'))\n",
    "model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_4 = model_4.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_4.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_4.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Reduced GRU with More Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Sequential()\n",
    "model_5.add(embedding_layer)\n",
    "model_5.add(GRU(64, \n",
    "               dropout = 0.3, \n",
    "               recurrent_dropout = 0.5,\n",
    "                 return_sequences = True))\n",
    "model_5.add(GRU(32,\n",
    "                dropout = 0.2,\n",
    "                recurrent_dropout = 0.5))\n",
    "model_5.add(Dense(3, activation='softmax'))\n",
    "model_5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_5 = model_5.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_5.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_5.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "# Bidirectional RNNs\n",
    "model_6 = Sequential()\n",
    "model_6.add(embedding_layer)\n",
    "model_6.add(Bidirectional(LSTM(64,\n",
    "                              dropout=0.2,\n",
    "                              recurrent_dropout=0.5)))\n",
    "model_6.add(Dense(3,activation='softmax'))\n",
    "model_6.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_6 = model_6.fit(X_train, y_train,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_6.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_6.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <font color=\"#004D7F\">Comparación de Modelos</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clave para una comparación equitativa de los algoritmos de ML es garantizar que cada algoritmo se evalúe de la misma manera en los mismos datos. Los algoritmos se comparan en un único conjunto de datos:\n",
    "* Logistic Regression.\n",
    "* Linear Discriminant Analysis.\n",
    "* k-Nearest Neighbors.\n",
    "* Classification and Regression Trees. \n",
    "* Naive Bayes.\n",
    "* Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "models.append((\"Simple LSTM Model with regularization, increase dimensionality\",  lstm_mod1.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"LSTM with regularization, reduce dimensionality\",  lstm_mod2.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"LSTM Layer Stacking\",  model_3.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"GRU Layer Stacking\", model_4.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"Reduced GRU with More Regularization\",  model_5.evaluate(X_train, y_train, verbose=False)))\n",
    "models.append((\"Bidirectional RNN\",  model_6.evaluate(X_train, y_train, verbose=False)))\n",
    "\n",
    "results=[]\n",
    "names=[]\n",
    "\n",
    "\n",
    "for name, model in models: \n",
    "    loss, accuracy = model\n",
    "    results.append(accuracy)\n",
    "    names.append(name)\n",
    "    print(f\"Accuracy: {name}: {cv_results.mean()*100.0:,.2f}%  ({cv_results.std()*100.0:,.2f}) %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
